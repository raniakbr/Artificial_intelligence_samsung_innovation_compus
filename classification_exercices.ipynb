{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "classification_exercices.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b3ed8e708b643839dc49c9e4479760f0fed46f4af64d471bc476f8b8b680b9fd"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Softmax regression (multi-class logistic regression):"
      ],
      "metadata": {
        "id": "CK_S5X_mDUGJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "source": [
        "import warnings \r\n",
        "\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.preprocessing import scale\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "tf.compat.v1.disable_eager_execution()\r\n"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-27T23:11:55.143918Z",
          "start_time": "2021-08-27T23:11:33.536051Z"
        },
        "id": "3fRWxjWIDUGT",
        "outputId": "34b0471e-4809-44c9-9fed-8807757012f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1. Read in the data:"
      ],
      "metadata": {
        "id": "9JnvsWClDUGX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "source": [
        "# We will use Iris data.\r\n",
        "# 4 explanatory variables.\r\n",
        "# 3 classes for the response variable.\r\n",
        "data_raw = load_iris()\r\n",
        "data_raw.keys()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-27T23:11:55.301094Z",
          "start_time": "2021-08-27T23:11:55.178534Z"
        },
        "id": "-90xdf2BDUGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "# Print out the description.\r\n",
        "# print(data_raw['DESCR'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "416lWJFrDUGZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "source": [
        "X = data_raw['data']\r\n",
        "y = data_raw['target']"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-27T23:11:55.410032Z",
          "start_time": "2021-08-27T23:11:55.397041Z"
        },
        "id": "a2X7U7giDUGa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "source": [
        "# Check the shape.\r\n",
        "print(X.shape)\r\n",
        "print(y.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4)\n",
            "(150,)\n"
          ]
        }
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-27T23:11:55.505165Z",
          "start_time": "2021-08-27T23:11:55.492191Z"
        },
        "id": "-2EdTRWDDUGb",
        "outputId": "68d1c50d-5e54-420a-e0fc-aba5f1db2dad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2. Data pre-processing:"
      ],
      "metadata": {
        "id": "Nsb4A9clDUGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "source": [
        "# One-Hot-Encoding.\r\n",
        "y = np.array(pd.get_dummies(y, drop_first=False))               # drop_frist = False for one-hot-encoding.\r\n",
        "y.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-27T23:12:12.479972Z",
          "start_time": "2021-08-27T23:12:12.318691Z"
        },
        "id": "LdRX5ZxjDUGf",
        "outputId": "13809c80-6805-41d3-8022-d9910cc83cd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "source": [
        "# Scaling\r\n",
        "X = scale(X)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-27T23:12:14.027660Z",
          "start_time": "2021-08-27T23:12:13.966594Z"
        },
        "id": "yGk1ICDQDUGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\r\n",
        "n_train_size = y_train.shape[0]"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-27T23:12:14.510803Z",
          "start_time": "2021-08-27T23:12:14.477824Z"
        },
        "id": "h_RnAH7MDUGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3. Do the necessary definitions:"
      ],
      "metadata": {
        "id": "LYOe_WGHDUGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "source": [
        "batch_size = 100                                # Size of each (mini) batch.\r\n",
        "n_epochs  = 30000                               # Number of epochs.\r\n",
        "learn_rate = 0.05"
      ],
      "outputs": [],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-27T23:12:18.738513Z",
          "start_time": "2021-08-27T23:12:18.726500Z"
        },
        "id": "eJ1pI2UkDUGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the weights and bias , we have to make sure to set the approriate weights , for the sake of simplicity we fill them with ones , that means you use ``tf.ones()``"
      ],
      "metadata": {
        "id": "InPau-11DUGm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "source": [
        "W = tf.Variable(tf.ones([4,3]))                 # Initial value of the weights = 1.\r\n",
        "b = tf.Variable(tf.ones([3]))                   # Initial value of the bias = 1.\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "MrYWZfWrDUGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "define placeholders for x and y "
      ],
      "metadata": {
        "id": "S3krtbUnDUGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "source": [
        "# Number of rows not specified. Number of columns = numbmer of X variables please use 4.\r\n",
        "X_ph =tf.compat.v1.placeholder(tf.float32, shape=([None,4]))\r\n",
        "# Number of rows not specified. Number of columns = number of classes of the y variable = 3.\r\n",
        "y_ph = tf.compat.v1.placeholder(tf.float32, shape=([None,3]))\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "RYTSBNoVDUGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "define the model , recall that $y\\_model = X*W + b$"
      ],
      "metadata": {
        "id": "ZOFsKeVDDUGo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "source": [
        "# Model.\r\n",
        "# Not strictly necessary to apply the softmax activation. => in the end we will apply argmax() function to predict the label!\r\n",
        "# y_model = tf.nn.softmax(tf.matmul(X_ph, W) + b)\r\n",
        "# The following should contain what is inside the softmax function\r\n",
        "y_model = tf.nn.softmax(tf.matmul(X_ph, W) + b)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "gjBYZ_6ODUGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "defin the loss function , recall that we are dealing with a multi-class classification problem ,thus we will need to work with softmax and cross_entropy (hint : use ``tf.nn.softmax_cross_entropy_with_logits_v2`` )"
      ],
      "metadata": {
        "id": "vlXA-3PlDUGp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "loss  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ph, logits=y_model))     # Loss = cross entropy. "
      ],
      "outputs": [],
      "metadata": {
        "id": "wIedf5vNDUGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "define your optimizer , use gradient descent"
      ],
      "metadata": {
        "id": "hb5tSjdGDUGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "source": [
        "optimizer = tf.compat.v1.train.MomentumOptimizer(\r\n",
        "    learning_rate=0.001, momentum=0.9)\r\n",
        "train = optimizer.minimize(loss)\r\n",
        "init = tf.global_variables_initializer()\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "9ZXxcRYBDUGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4. Training and Testing:"
      ],
      "metadata": {
        "id": "B3ZlOoeeDUGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- train your model this time using batches , (hint : make sure to use slicing and  ``np.random.choice`` to get a sample of index with a size equal to ``batch_size``.\n",
        "- since we are working with batches ,at each iteration the my_feed dictionary will change according to batches\n"
      ],
      "metadata": {
        "id": "fA8FXLGEDUGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "source": [
        "with tf.Session() as sess:\r\n",
        "        # Variables initialization.\r\n",
        "        sess.run(init)\r\n",
        "        # Training.\r\n",
        "        for i in range(n_epochs):\r\n",
        "            #define your batches here in two line of codes #\r\n",
        "            index = np.random.choice(len(X_train), batch_size)\r\n",
        "            x_b = []\r\n",
        "            y_b = []\r\n",
        "            for j in index:\r\n",
        "                x_b.append(X_train[j])\r\n",
        "                y_b.append(y_train[j])\r\n",
        "            my_feed = {X_ph: x_b, y_ph: y_b}\r\n",
        "            #run training using sess.run as usual#\r\n",
        "            sess.run(train, feed_dict=my_feed)\r\n",
        "            if (i + 1) % 2000 == 0:\r\n",
        "                   # Print the step number at every multiple of 2000.\r\n",
        "                   print(\"Step : {}\".format(i + 1))\r\n",
        "        # Testing.\r\n",
        "        # In argmax(), axis=1 means horizontal direction.\r\n",
        "        correct_predictions = tf.equal(tf.argmax(y_ph, axis=1), tf.argmax(y_model, axis=1))\r\n",
        "        # Recast the Boolean as float32 first. Then calculate the mean.\r\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\r\n",
        "        accuracy_value = sess.run(accuracy, feed_dict={X_ph: X_test, y_ph: y_test})             # Actually run the test with the test data.\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step : 2000\n",
            "Step : 4000\n",
            "Step : 6000\n",
            "Step : 8000\n",
            "Step : 10000\n",
            "Step : 12000\n",
            "Step : 14000\n",
            "Step : 16000\n",
            "Step : 18000\n",
            "Step : 20000\n",
            "Step : 22000\n",
            "Step : 24000\n",
            "Step : 26000\n",
            "Step : 28000\n",
            "Step : 30000\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the testing result."
      ],
      "metadata": {
        "id": "yNkrj1jLDUGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "source": [
        "print(\"Accuracy = {:5.3f}\".format(accuracy_value))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.933\n"
          ]
        }
      ],
      "metadata": {
        "id": "GcypDDbVDUGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "ZRM9mG4mDUGt"
      }
    }
  ]
}