{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "name": "problem_0801.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b3ed8e708b643839dc49c9e4479760f0fed46f4af64d471bc476f8b8b680b9fd"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Quiz #0801"
      ],
      "metadata": {
        "id": "FsxL_e2JnusZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"Text Classification with Keras\""
      ],
      "metadata": {
        "id": "TGLZR7ZFnusi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "import os\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from sklearn.datasets import load_files\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, SimpleRNN, LSTM, Embedding\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from keras.preprocessing import sequence\r\n",
        "import string\r\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "STOPWORDS = set(stopwords.words('english'))\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Rania\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Rania\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "metadata": {
        "id": "B9sECm6xnusk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer the following question by providing Python code:"
      ],
      "metadata": {
        "id": "-PDiT-Kjnusm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1). Read in the movie review data from Cornell CS department. Carry out the EDA. <br>\n",
        "- The data can be found [here](https://www.cs.cornell.edu/people/pabo/movie-review-data). <br>\n",
        "- Download the “polarity dataset” and unzip. <br>\n",
        "- Under the \"txt_sentoken” folder, there are “pos” and “neg\" subfolders. <br>"
      ],
      "metadata": {
        "id": "J5MLkOq5nusn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "source": [
        "reviews = load_files('txt_sentoken/')\r\n",
        "my_docs, y = reviews.data, reviews.target\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "dWFGNFxinuso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2). Carry out the data preprocessing: <br>\n",
        "- Cleaning.\n",
        "- Stopword removal."
      ],
      "metadata": {
        "id": "Ydn4vg2Qnusp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "source": [
        "def clean_text(text):\r\n",
        "    text = str(text)\r\n",
        "    text = text.lower()\r\n",
        "    text = re.sub('\\[.*?\\]', '', text)\r\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\r\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\r\n",
        "    text = re.sub('\\n', '', text)\r\n",
        "    text = text.replace('x', '')\r\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "corpus= list(map(lambda x: clean_text(x), list(my_docs)))\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "pSYaAnxJnusr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3). Carry out label encoding by integers (required form by Keras):"
      ],
      "metadata": {
        "id": "i-OgF_1knuss"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "source": [
        "# Make a dictionary with the top words.\r\n",
        "n_words = 2000  \r\n",
        "words = []\r\n",
        "for i in range(len(corpus)):\r\n",
        "    words += nltk.word_tokenize(corpus[i])\r\n",
        "top_words = pd.Series(words).value_counts().index\r\n",
        "top_words = top_words[0:n_words]                     # Apply a limitation.\r\n",
        "my_dict = {}\r\n",
        "my_dict_inv = {}\r\n",
        "nbr = list(pd.Series(words).value_counts())\r\n",
        "for i in range(len(top_words)):\r\n",
        "    my_dict_inv[top_words[i]] = i\r\n",
        "    my_dict[top_words[i]] = nbr[i]\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "nYkJ91Ttnust"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "source": [
        "# Convert the corpus into the label encoded form.\r\n",
        "corpus_int =[]\r\n",
        "for i in range(len(corpus)):\r\n",
        "    words = nltk.word_tokenize(corpus[i])\r\n",
        "    words2int = []\r\n",
        "    for x in words:\r\n",
        "        if x in my_dict:\r\n",
        "            words2int += [my_dict_inv[x]]\r\n",
        "    corpus_int.append(words2int)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "nnHCdPVTpX53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4). Prepare the data for AI: <br>\n",
        "- Apply the padding.\n",
        "- Split the data into training and testing."
      ],
      "metadata": {
        "id": "mNEnRmd9nusu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "source": [
        "X = np.array(corpus_int)\r\n",
        "y = np.array(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Rania\\AppData\\Local\\Temp/ipykernel_13092/2831807499.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  X = np.array(corpus_int)\n"
          ]
        }
      ],
      "metadata": {
        "id": "zwbC4KvRnusv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "source": [
        "# Padding: newswire lengths are uniformly matched to maxlen.\r\n",
        "X = sequence.pad_sequences(X, maxlen = 100)\r\n",
        "X.shape\r\n",
        "# y is already binary. Thus, there is no need to covert to the one-hot-encoding scheme."
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ],
      "metadata": {
        "id": "PMNTLBDuoJu4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "source": [
        "#split the data into training and testing\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(\r\n",
        "    X, y, test_size=0.2)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "zfnO5zW4oM4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5). Define the AI model (Embedding + LSTM):"
      ],
      "metadata": {
        "id": "qtHW6J1unusv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "source": [
        "x_train.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "source": [
        "n_neurons = 100                    # Neurons within each memory cell.\r\n",
        "n_input = 500                     # Dimension of the embeding space. "
      ],
      "outputs": [],
      "metadata": {
        "id": "ezoNRvm9nusw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "source": [
        "\r\n",
        "my_model = Sequential()\r\n",
        "my_model.add(Embedding(2000, n_input, input_length=100))\r\n",
        "my_model.add(LSTM(n_neurons ))\r\n",
        "my_model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "\r\n",
        "print(my_model.summary())\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 500)          1000000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               240400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,240,501\n",
            "Trainable params: 1,240,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "metadata": {
        "id": "9r6EBhLTonps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6). Define the optimizer and compile the model:"
      ],
      "metadata": {
        "id": "1tw-fUxvnusx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "source": [
        "n_epochs = 15                      # Number of epochs.\r\n",
        "batch_size = 100                    # Size of each batch.\r\n",
        "learn_rate = 0.0001 "
      ],
      "outputs": [],
      "metadata": {
        "id": "_SthDq7unusx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7). Train the model and visualize the summary:"
      ],
      "metadata": {
        "id": "z1kSE53Gnusy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "source": [
        "my_model.compile(loss='binary_crossentropy',\r\n",
        "                 optimizer='RMSprop', metrics=['accuracy'])\r\n",
        "history = my_model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size,validation_split=0.1)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "15/15 [==============================] - 10s 543ms/step - loss: 0.6948 - accuracy: 0.5653 - val_loss: 0.6319 - val_accuracy: 0.7000\n",
            "Epoch 2/15\n",
            "15/15 [==============================] - 8s 530ms/step - loss: 0.4739 - accuracy: 0.8292 - val_loss: 0.4107 - val_accuracy: 0.8125\n",
            "Epoch 3/15\n",
            "15/15 [==============================] - 8s 531ms/step - loss: 0.2856 - accuracy: 0.8840 - val_loss: 0.5414 - val_accuracy: 0.7625\n",
            "Epoch 4/15\n",
            "15/15 [==============================] - 8s 549ms/step - loss: 0.1808 - accuracy: 0.9312 - val_loss: 0.5365 - val_accuracy: 0.8062\n",
            "Epoch 5/15\n",
            "15/15 [==============================] - 8s 527ms/step - loss: 0.1090 - accuracy: 0.9674 - val_loss: 0.5407 - val_accuracy: 0.7812\n",
            "Epoch 6/15\n",
            "15/15 [==============================] - 8s 526ms/step - loss: 0.0506 - accuracy: 0.9889 - val_loss: 0.5959 - val_accuracy: 0.7875\n",
            "Epoch 7/15\n",
            "15/15 [==============================] - 8s 528ms/step - loss: 0.0666 - accuracy: 0.9792 - val_loss: 0.6330 - val_accuracy: 0.7812\n",
            "Epoch 8/15\n",
            "15/15 [==============================] - 8s 524ms/step - loss: 0.0134 - accuracy: 0.9979 - val_loss: 0.8707 - val_accuracy: 0.7812\n",
            "Epoch 9/15\n",
            "15/15 [==============================] - 8s 524ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8954 - val_accuracy: 0.8188\n",
            "Epoch 10/15\n",
            "15/15 [==============================] - 8s 527ms/step - loss: 5.9497e-04 - accuracy: 1.0000 - val_loss: 1.0312 - val_accuracy: 0.8000\n",
            "Epoch 11/15\n",
            "15/15 [==============================] - 8s 538ms/step - loss: 2.6711e-04 - accuracy: 1.0000 - val_loss: 1.1807 - val_accuracy: 0.7937\n",
            "Epoch 12/15\n",
            "15/15 [==============================] - 8s 551ms/step - loss: 1.2078e-04 - accuracy: 1.0000 - val_loss: 1.3164 - val_accuracy: 0.7937\n",
            "Epoch 13/15\n",
            "15/15 [==============================] - 8s 567ms/step - loss: 5.7570e-05 - accuracy: 1.0000 - val_loss: 1.4895 - val_accuracy: 0.7937\n",
            "Epoch 14/15\n",
            "15/15 [==============================] - 9s 590ms/step - loss: 3.0101e-05 - accuracy: 1.0000 - val_loss: 1.6122 - val_accuracy: 0.8000\n",
            "Epoch 15/15\n",
            "15/15 [==============================] - 9s 594ms/step - loss: 1.3291e-05 - accuracy: 1.0000 - val_loss: 1.7796 - val_accuracy: 0.7812\n"
          ]
        }
      ],
      "metadata": {
        "id": "k2OaiwlZh9zU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8). Display the test result (accuracy):"
      ],
      "metadata": {
        "id": "1Yw9Eh_5nusy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "source": [
        "score = my_model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print('Accuracy: {:0.2f}'.format(score[1]))\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.77\n"
          ]
        }
      ],
      "metadata": {
        "id": "Ypva7wC6nusz"
      }
    }
  ]
}